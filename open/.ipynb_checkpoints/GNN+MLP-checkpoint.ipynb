{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff09e1b",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import\" data-toc-modified-id=\"Import-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import</a></span></li><li><span><a href=\"#Pre-Processing\" data-toc-modified-id=\"Pre-Processing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Pre-Processing</a></span></li><li><span><a href=\"#Custom-Dataset\" data-toc-modified-id=\"Custom-Dataset-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Custom Dataset</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Model</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Inference\" data-toc-modified-id=\"Inference-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Inference</a></span></li><li><span><a href=\"#Submission\" data-toc-modified-id=\"Submission-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Submission</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pYzhJrEibIlq",
   "metadata": {
    "id": "pYzhJrEibIlq"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f487ab4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.039Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_smiles\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import PandasTools, AllChem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adfbbec",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.041Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714740f2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.041Z"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(42) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afd5126",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd052629",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.042Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv').drop(columns=[\"id\"])\n",
    "test = pd.read_csv('./test.csv').drop(columns=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a2ae9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.043Z"
    }
   },
   "outputs": [],
   "source": [
    "# 사용할 column만 추출\n",
    "train = train[['SMILES','MLM', 'HLM', 'AlogP', 'Molecular_Weight', 'Num_H_Acceptors', 'Num_H_Donors', 'Molecular_PolarSurfaceArea']]\n",
    "test = test[['SMILES', 'AlogP', 'Molecular_Weight', 'Num_H_Acceptors', 'Num_H_Donors', 'Molecular_PolarSurfaceArea']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72a8b15",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.044Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imputer = IterativeImputer(estimator=RandomForestRegressor(n_jobs=-1), random_state=42)\n",
    "imputer.fit_transform(train.drop(columns=['SMILES', 'MLM', 'HLM']))\n",
    "imputer.transform(test.drop(columns='SMILES'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b22d83d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0d289fc",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145353ba",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.045Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, target, is_test=False):\n",
    "        self.df = df\n",
    "        self.target = target # HLM or MLM\n",
    "        self.is_test = is_test # train,valid / test\n",
    "        self.scaler = MinMaxScaler()\n",
    "        \n",
    "        if not self.is_test: \n",
    "            self.graph = self.smiles2mol(df['SMILES'])\n",
    "            self.others = self.scaler.fit_transform(df.iloc[:,3:])\n",
    "\n",
    "        else: # valid or test\n",
    "            self.graph = self.smiles2mol(df['SMILES'])\n",
    "            self.others = self.scaler.transform(df.iloc[:,1:])\n",
    "\n",
    "    def smiles2mol(self, smiles_list):\n",
    "        graph_list = []\n",
    "        for smiles in smiles_list:\n",
    "            graph_data = from_smiles(smiles)\n",
    "\n",
    "            graph_data.smiles = None\n",
    "            graph_data.edge_attr = None\n",
    "\n",
    "            graph_list.append(graph_data)\n",
    "        return graph_list            \n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        graph = self.graph[index]\n",
    "        others = self.others[index]\n",
    "        if not self.is_test: # test가 아닌 경우(label 존재)\n",
    "            label = self.df[self.target][index]\n",
    "            return graph, torch.tensor(others).float(), torch.tensor(label).float().unsqueeze(dim=-1) # feature, label\n",
    "\n",
    "        else: # test인 경우\n",
    "            return graph, torch.tensor(others).float() # feature\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c600a122",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.045Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_MLM = CustomDataset(df=train, target='MLM', is_test=False)\n",
    "train_HLM = CustomDataset(df=train, target='HLM', is_test=False)\n",
    "\n",
    "input_size1 = 32\n",
    "input_size2 = train_MLM.others.shape[1]\n",
    "print(input_size1, input_size2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb36a46",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.046Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "CFG = {'BATCH_SIZE': 256,\n",
    "       'EPOCHS': 100,\n",
    "       'INPUT_SIZE1': input_size1,\n",
    "       'INPUT_SIZE2': input_size2,\n",
    "       'HIDDEN_SIZE': 512,\n",
    "       'OUTPUT_SIZE': 1,\n",
    "       'DROPOUT_RATE': 0.8,\n",
    "       'LEARNING_RATE': 1e-3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090fa2f2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.047Z"
    }
   },
   "outputs": [],
   "source": [
    "# train,valid split\n",
    "train_MLM_dataset, valid_MLM_dataset = train_test_split(train_MLM, test_size=0.2, random_state=42)\n",
    "train_HLM_dataset, valid_HLM_dataset = train_test_split(train_HLM, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe86f36f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.048Z"
    }
   },
   "outputs": [],
   "source": [
    "train_MLM_loader = DataLoader(dataset=train_MLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=True)\n",
    "\n",
    "valid_MLM_loader = DataLoader(dataset=valid_MLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)\n",
    "\n",
    "\n",
    "train_HLM_loader = DataLoader(dataset=train_HLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=True)\n",
    "\n",
    "valid_HLM_loader = DataLoader(dataset=valid_HLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8CHzFfvrbnOM",
   "metadata": {
    "id": "8CHzFfvrbnOM"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68141433",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.049Z"
    },
    "id": "AWUlJIGf22DO"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size2, hidden_size, dropout_rate, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # fc 레이어 3개와 출력 레이어\n",
    "        self.fc1 = nn.Linear(input_size2, hidden_size) \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # 정규화\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.ln3 = nn.LayerNorm(hidden_size)        \n",
    "        self.ln4 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # 활성화 함수\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.fc1(x)\n",
    "        out = self.ln1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.ln2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "#         out = self.fc3(out)\n",
    "#         out = self.ln3(out)\n",
    "#         out = self.activation(out)\n",
    "#         out = self.dropout(out)\n",
    "\n",
    "#         out = self.fc4(out)\n",
    "#         out = self.ln4(out)\n",
    "#         out = self.activation(out)\n",
    "#         out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bfb402",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.049Z"
    }
   },
   "outputs": [],
   "source": [
    "# GNN 모델 정의\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, input_size1, hidden_size, dropout_rate, output_size):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_size1, hidden_size)\n",
    "        self.conv2 = GCNConv(hidden_size, hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # 활성화 함수\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "     \n",
    "    def forward(self, x, batch):\n",
    "        out, edge_index = x, x.edge_index\n",
    "\n",
    "        # 첫 번째 Graph Convolution 레이어 적용\n",
    "        out = self.conv1(out, edge_index)\n",
    "        out = self.bn1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # 두 번째 Graph Convolution 레이어 적용\n",
    "        out = self.conv2(out, edge_index)\n",
    "        out = self.bn2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = global_mean_pool(out, batch) # read-out layer\n",
    "        \n",
    "        out = self.fc_out(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c175bbca",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.050Z"
    }
   },
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, input_size1, input_size2, hidden_size, drop_rate, output_size):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.GNN_extractor = GNN(input_size1, hidden_size, drop_rate, output_size)\n",
    "        self.Net_extractor = Net(input_size2, hidden_size, drop_rate, output_size)\n",
    "#         self.classifier = nn.Linear(in_features=hidden_size*2, out_features=output_size)\n",
    "        self.classifier = nn.Linear(in_features=output_size*2, out_features=output_size)\n",
    "\n",
    "    def forward(self, graph, others, batch):\n",
    "        GNN_feature = self.GNN_extractor(graph, batch)\n",
    "        Net_feature = self.Net_extractor(others)\n",
    "        feature = torch.cat([GNN_feature, Net_feature], dim=-1)\n",
    "        output = self.classifier(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9IlcjfOB22DO",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.051Z"
    },
    "id": "9IlcjfOB22DO"
   },
   "outputs": [],
   "source": [
    "model_MLM = ClassificationModel(CFG['INPUT_SIZE1'],CFG['INPUT_SIZE2'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE'])\n",
    "model_HLM = ClassificationModel(CFG['INPUT_SIZE1'],CFG['INPUT_SIZE2'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IuQe4Za322DP",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.051Z"
    },
    "id": "IuQe4Za322DP"
   },
   "outputs": [],
   "source": [
    "# criterion = nn.MSELoss()\n",
    "optimizer_MLM = torch.optim.Adam(model_MLM.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "optimizer_HLM = torch.optim.Adam(model_HLM.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "# optimizer_MLM = torch.optim.SGD(model_MLM.parameters(), lr=CFG['LEARNING_RATE'], momentum=0.9)\n",
    "# optimizer_HLM = torch.optim.SGD(model_HLM.parameters(), lr=CFG['LEARNING_RATE'], momentum=0.9)\n",
    "scheduler_MLM = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_MLM, mode='min', factor=0.5, patience=40, threshold_mode='abs', min_lr=0, eps=1e-8, verbose=True)\n",
    "scheduler_HLM = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_HLM, mode='min', factor=0.5, patience=40, threshold_mode='abs', min_lr=0, eps=1e-8, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e032e346",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17df6b3-16c9-44dd-b0fd-ffb501fee749",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.052Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    \n",
    "    \n",
    "    for epoch in range(CFG['EPOCHS']):\n",
    "        running_loss = 0\n",
    "        for inputs, others, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            others = others.to(device)\n",
    "            targets = targets.to(device)\n",
    "            batch = inputs.batch.to(device)\n",
    "            \n",
    "            output = model(inputs, others, batch)\n",
    "#             print(1)\n",
    "            \n",
    "            loss = criterion(output, targets)\n",
    "#             print(2)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "\n",
    "            \n",
    "        if epoch % 100 == 0:\n",
    "            \n",
    "            val_loss = validation(model, criterion, val_loader, device)\n",
    "            \n",
    "            print(f'Epoch: {epoch}, Train Loss: {running_loss/len(train_loader)}, Valid Loss: {val_loss/len(valid_HLM_loader)}')\n",
    "            \n",
    "            model.train()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "            if scheduler.num_bad_epochs > scheduler.patience:\n",
    "                print(f'Early stopping at epoch {epoch}...')\n",
    "                break\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b1c524-89fb-4ce8-a49f-067fd489f84a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.053Z"
    }
   },
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, others, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            others = others.to(device)\n",
    "            targets = targets.to(device)\n",
    "            batch = inputs.batch.to(device)\n",
    "            \n",
    "            output = model(inputs, others, batch)\n",
    "            loss = criterion(output, targets)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    \n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda577c7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.054Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Training Start: MLM\")\n",
    "# model_MLM = train(train_MLM_loader, valid_MLM_loader, model_MLM, criterion, optimizer_MLM, epochs=CFG['EPOCHS'])\n",
    "model_MLM = train(model_MLM, optimizer_MLM, train_MLM_loader, valid_MLM_loader, scheduler_MLM, device)\n",
    "print(\"Training Start: HLM\")\n",
    "# model_HLM = train(train_HLM_loader, valid_HLM_loader, model_HLM, criterion, optimizer_HLM, epochs=CFG['EPOCHS'])\n",
    "model_HLM = train(model_HLM, optimizer_HLM, train_HLM_loader, valid_HLM_loader, scheduler_HLM, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39bbce5",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbc3181",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.055Z"
    }
   },
   "outputs": [],
   "source": [
    "test_MLM = CustomDataset(df=test, target=None, is_test=True)\n",
    "test_HLM = CustomDataset(df=test, target=None, is_test=True)\n",
    "\n",
    "test_MLM_loader = DataLoader(dataset=test_MLM,\n",
    "                             batch_size=CFG['BATCH_SIZE'],\n",
    "                             shuffle=False)\n",
    "\n",
    "test_HLM_loader = DataLoader(dataset=test_HLM,\n",
    "                             batch_size=CFG['BATCH_SIZE'],\n",
    "                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d8a4e4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.055Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference(test_loader, model):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, others in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            others = others.to(device)\n",
    "            output = model(inputs, others)\n",
    "            preds.extend(output.cpu().numpy().flatten().tolist())\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9794de7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.056Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions_MLM = inference(test_MLM_loader, model_MLM)\n",
    "predictions_HLM = inference(test_HLM_loader, model_HLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb42d64",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e574c0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.056Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./sample_submission.csv')\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569d8575",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.057Z"
    }
   },
   "outputs": [],
   "source": [
    "submission['MLM'] = predictions_MLM\n",
    "submission['HLM'] = predictions_HLM\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaf46fc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-17T16:35:08.057Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv('baseline_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb1c65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "804.705872px",
    "left": "213px",
    "top": "118.698524px",
    "width": "211.985291px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "88a5da79f9030d36a713e3ceec9ed9a47a216907c035af9944c458137c4e5cb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
