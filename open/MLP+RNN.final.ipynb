{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff09e1b",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import\" data-toc-modified-id=\"Import-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import</a></span></li><li><span><a href=\"#Pre-Processing\" data-toc-modified-id=\"Pre-Processing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Pre-Processing</a></span></li><li><span><a href=\"#Custom-Dataset\" data-toc-modified-id=\"Custom-Dataset-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Custom Dataset</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Model</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Inference\" data-toc-modified-id=\"Inference-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Inference</a></span></li><li><span><a href=\"#Submission\" data-toc-modified-id=\"Submission-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Submission</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pYzhJrEibIlq",
   "metadata": {
    "id": "pYzhJrEibIlq"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f487ab4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T02:10:38.543340Z",
     "start_time": "2023-09-23T02:10:36.652027Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import PandasTools, AllChem, rdMolDescriptors, MolFromSmiles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6adfbbec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T02:10:38.559245Z",
     "start_time": "2023-09-23T02:10:38.544761Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714740f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T02:10:38.563930Z",
     "start_time": "2023-09-23T02:10:38.560386Z"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(42) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afd5126",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b48c6e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T02:12:01.353644Z",
     "start_time": "2023-09-23T02:10:38.565384Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "100%|██████████████████████████████████████| 3498/3498 [00:09<00:00, 356.08it/s]\n",
      "100%|████████████████████████████████████████| 483/483 [00:01<00:00, 363.24it/s]\n",
      "100%|███████████████████████████████████████| 3498/3498 [00:55<00:00, 62.64it/s]\n",
      "100%|█████████████████████████████████████████| 483/483 [00:07<00:00, 63.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 14, 15, 16, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191] 180\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 14, 15, 16, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191] 180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from mendeleev.fetch import fetch_table\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import (Descriptors,\n",
    "                        Lipinski,\n",
    "                        Crippen,\n",
    "                        rdMolDescriptors,\n",
    "                        MolFromSmiles,\n",
    "                        AllChem,\n",
    "                        PandasTools)\n",
    "\n",
    "from torch import Tensor\n",
    "from torch_geometric.utils.sparse import dense_to_sparse\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from rdkit import DataStructs\n",
    "from deepchem import feat\n",
    "\n",
    "archieve = ['seyonec/PubChem10M_SMILES_BPE_450k', \"DeepChem/ChemBERTa-77M-MTR\", 'seyonec/ChemBERTa-zinc-base-v1', 'seyonec/ChemBERTa_zinc250k_v2_40k']\n",
    "chosen = archieve[2]\n",
    "\n",
    "class Chemical_feature_generator():\n",
    "    def __init__(self) -> None:\n",
    "        mendeleev_atomic_f = ['atomic_radius', 'atomic_radius_rahm', 'atomic_volume', 'atomic_weight', 'c6', 'c6_gb', \n",
    "                        'covalent_radius_cordero', 'covalent_radius_pyykko', 'covalent_radius_pyykko_double', 'covalent_radius_pyykko_triple', \n",
    "                        'density', 'dipole_polarizability', 'dipole_polarizability_unc', 'electron_affinity', 'en_allen', 'en_ghosh', 'en_pauling', \n",
    "                        'heat_of_formation', 'is_radioactive', 'molar_heat_capacity', 'specific_heat_capacity', 'vdw_radius']\n",
    "                        # Others are fine\n",
    "                        # Heat of Formation: This reflects the energy associated with the formation of a molecule and might indirectly impact metabolic reactions.\n",
    "                        # Is Radioactive: This binary property may not be directly relevant to metabolic stability.\n",
    "                        # Molar Heat Capacity, Specific Heat Capacity: These properties relate to heat transfer but might not be directly tied to metabolic stability.\n",
    "        self.mendeleev_atomic_f_table = fetch_table('elements')[mendeleev_atomic_f]\n",
    "        \n",
    "        self.DMPNNFeaturizer = feat.DMPNNFeaturizer()\n",
    "        self.Mol2VecFingerprint = feat.Mol2VecFingerprint()\n",
    "        self.BPSymmetryFunctionInput = feat.BPSymmetryFunctionInput(max_atoms=150)\n",
    "        \n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(chosen)\n",
    "        \n",
    "\n",
    "    def get_atomic_features(self,atom):\n",
    "\n",
    "        atomic_num = atom.GetAtomicNum() - 1 # -1 is offset\n",
    "        mendel_atom_f = self.mendeleev_atomic_f_table.loc[atomic_num]\n",
    "\n",
    "        mendel_atom_f = mendel_atom_f.to_numpy().astype(np.float32)\n",
    "\n",
    "        rdkit_atom_f = [atom.GetDegree(),\n",
    "                atom.GetTotalDegree(),\n",
    "                atom.GetFormalCharge(),\n",
    "                atom.GetIsAromatic()*1.,\n",
    "                atom.GetNumImplicitHs(),\n",
    "                atom.GetNumExplicitHs(),\n",
    "                atom.GetTotalNumHs(),\n",
    "                atom.GetNumRadicalElectrons(),\n",
    "                atom.GetImplicitValence(),\n",
    "                atom.GetExplicitValence(),\n",
    "                atom.GetTotalValence(),\n",
    "                atom.IsInRing()*1.]\n",
    "        \n",
    "        return mendel_atom_f, rdkit_atom_f\n",
    "    \n",
    "    def get_molecular_features(self, mol):\n",
    "        ## 1. Molecular Descriptors 5\n",
    "        MolWt = Descriptors.MolWt(mol)\n",
    "        HeavyAtomMolWt = Descriptors.HeavyAtomMolWt(mol)\n",
    "        NumValenceElectrons = Descriptors.NumValenceElectrons(mol)\n",
    "        MolMR = Crippen.MolMR(mol)\n",
    "        MolLogP = Crippen.MolLogP(mol)\n",
    "\n",
    "        ## 2. Lipinski's Rule of Five 16\n",
    "        FractionCSP3 = Lipinski.FractionCSP3(mol)\n",
    "        HeavyAtomCount = Lipinski.HeavyAtomCount(mol)\n",
    "        NHOHCount = Lipinski.NHOHCount(mol)\n",
    "        NOCount = Lipinski.NOCount(mol)\n",
    "        NumAliphaticCarbocycles = Lipinski.NumAliphaticCarbocycles(mol)\n",
    "        NumAliphaticHeterocycles = Lipinski.NumAliphaticHeterocycles(mol)\n",
    "        NumAliphaticRings = Lipinski.NumAliphaticRings(mol)\n",
    "        NumAromaticCarbocycles = Lipinski.NumAromaticCarbocycles(mol)\n",
    "        NumAromaticHeterocycles = Lipinski.NumAromaticHeterocycles(mol)\n",
    "        NumAromaticRings = Lipinski.NumAromaticRings(mol)\n",
    "        NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
    "        NumHDonors = Lipinski.NumHDonors(mol)\n",
    "        NumHeteroatoms = Lipinski.NumHeteroatoms(mol)\n",
    "        NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
    "        RingCount = Lipinski.RingCount(mol)\n",
    "        CalcNumBridgeheadAtom = rdMolDescriptors.CalcNumBridgeheadAtoms(mol)\n",
    "\n",
    "        ## 3. Additional Features 11\n",
    "        ExactMolWt = Descriptors.ExactMolWt(mol)\n",
    "        NumRadicalElectrons = Descriptors.NumRadicalElectrons(mol)\n",
    "        NumSaturatedCarbocycles = Lipinski.NumSaturatedCarbocycles(mol)\n",
    "        NumSaturatedHeterocycles = Lipinski.NumSaturatedHeterocycles(mol)\n",
    "        NumSaturatedRings = Lipinski.NumSaturatedRings(mol)\n",
    "        CalcNumAmideBonds = rdMolDescriptors.CalcNumAmideBonds(mol)\n",
    "        CalcNumSpiroAtoms = rdMolDescriptors.CalcNumSpiroAtoms(mol)\n",
    "        \n",
    "        num_amion_groups = len(mol.GetSubstructMatches(MolFromSmiles(\"[NH2]\")))\n",
    "        num_ammonium_groups = len(mol.GetSubstructMatches(MolFromSmiles(\"[NH4+]\")))\n",
    "        num_sulfinic_acid_groups = len(mol.GetSubstructMatches(MolFromSmiles(\"[S](=O)(=O)\")))\n",
    "        num_alkoxy_groups = len(mol.GetSubstructMatches(MolFromSmiles('CO'))) \n",
    "        \n",
    "        return [MolWt,\n",
    "                HeavyAtomMolWt,\n",
    "                NumValenceElectrons,\n",
    "                FractionCSP3,\n",
    "                HeavyAtomCount,\n",
    "                NHOHCount,\n",
    "                NOCount,\n",
    "                NumAliphaticCarbocycles,\n",
    "                NumAliphaticHeterocycles,\n",
    "                NumAliphaticRings,\n",
    "                NumAromaticCarbocycles,\n",
    "                NumAromaticHeterocycles,\n",
    "                NumAromaticRings,\n",
    "                NumHAcceptors,\n",
    "                NumHDonors,\n",
    "                NumHeteroatoms,\n",
    "                NumRotatableBonds,\n",
    "                RingCount,\n",
    "                MolMR,\n",
    "                CalcNumBridgeheadAtom,\n",
    "                ExactMolWt,\n",
    "                NumRadicalElectrons,\n",
    "                NumSaturatedCarbocycles,\n",
    "                NumSaturatedHeterocycles,\n",
    "                NumSaturatedRings,\n",
    "                MolLogP,\n",
    "                CalcNumAmideBonds,\n",
    "                CalcNumSpiroAtoms,\n",
    "#                 num_carboxyl_groups,\n",
    "                num_amion_groups,\n",
    "                num_ammonium_groups,\n",
    "                num_sulfinic_acid_groups,\n",
    "                num_alkoxy_groups]\n",
    "\n",
    "    def get_molecule_fingerprints(self, mol):\n",
    "        ECFP12 = AllChem.GetHashedMorganFingerprint(mol, 6, nBits=2048) # 2048\n",
    "        ECFP6 = AllChem.GetHashedMorganFingerprint(mol, 3, nBits=2048) # 2048\n",
    "        \n",
    "        MACCS = Chem.rdMolDescriptors.GetMACCSKeysFingerprint(mol) # 167\n",
    "        RDK_fp = Chem.RDKFingerprint(mol) # 2048\n",
    "        Layer_fp = Chem.rdmolops.LayeredFingerprint(mol) # 2048\n",
    "        Pattern_fp = Chem.rdmolops.PatternFingerprint(mol) # 2048\n",
    "        \n",
    "        ecfp12 = np.zeros((1,), dtype=np.int8)\n",
    "        ecfp6 = np.zeros((1,), dtype=np.int8)\n",
    "        maccs = np.zeros((1,), dtype=np.int8)\n",
    "        rdk_fp = np.zeros((1,), dtype=np.int8)\n",
    "        layer_fp = np.zeros((1,), dtype=np.int8)\n",
    "        pattern_fp = np.zeros((1,), dtype=np.int8)\n",
    "        DataStructs.ConvertToNumpyArray(ECFP12, ecfp12)\n",
    "        DataStructs.ConvertToNumpyArray(ECFP6, ecfp6)\n",
    "        DataStructs.ConvertToNumpyArray(MACCS, maccs)\n",
    "        DataStructs.ConvertToNumpyArray(RDK_fp, rdk_fp)\n",
    "        DataStructs.ConvertToNumpyArray(Layer_fp, layer_fp)\n",
    "        DataStructs.ConvertToNumpyArray(Pattern_fp, pattern_fp)\n",
    "        return np.hstack([ecfp12, ecfp6, maccs, rdk_fp, layer_fp, pattern_fp]) \n",
    "    \n",
    "    def get_mol_feature_from_deepchem(self, smiles):\n",
    "        return self.Mol2VecFingerprint(smiles) # (1, 300)\n",
    "    \n",
    "    def encoder_smiles(self, smiles):\n",
    "        inputs = self.tokenizer.encode_plus(smiles, padding=True, return_tensors='pt', add_special_tokens=True)\n",
    "        return inputs['input_ids']\n",
    "    \n",
    "    def get_atomic_feature_from_deepchem(self, smiles):\n",
    "\n",
    "        DMPNN_F = self.DMPNNFeaturizer(smiles)[0].node_features\n",
    "        \n",
    "        return DMPNN_F\n",
    "\n",
    "    def generate_mol_atomic_features(self, smiles):\n",
    "\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "        # gathering atomic feature \n",
    "        mendel_atom_features = [] \n",
    "        rdkit_atom_features = [] \n",
    "        for atom in mol.GetAtoms():\n",
    "            mendel_atom_f, rdkit_atom_f = self.get_atomic_features(atom)\n",
    "\n",
    "            mendel_atom_features.append(mendel_atom_f)\n",
    "            rdkit_atom_features.append(rdkit_atom_f)\n",
    "\n",
    "        dc_atmoic = self.get_atomic_feature_from_deepchem(smiles)\n",
    "        atomic_features = np.concatenate([mendel_atom_features, rdkit_atom_features, dc_atmoic], axis=1, dtype=np.float32)\n",
    "        \n",
    "        return atomic_features\n",
    "    \n",
    "    def get_adj_matrix(self, smiles):\n",
    "        \n",
    "        mol = MolFromSmiles(smiles)\n",
    "        adj = Chem.rdmolops.GetAdjacencyMatrix(mol)\n",
    "        \n",
    "        edge_index, edge_attr = dense_to_sparse(Tensor(adj))\n",
    "            \n",
    "        return edge_index, edge_attr\n",
    "\n",
    "\n",
    "if __name__ == '__main__' : \n",
    "    import pandas as pd \n",
    "    from tqdm import tqdm\n",
    "    from deepchem.feat.molecule_featurizers import RDKitDescriptors\n",
    "    \n",
    "    train = pd.read_csv('./train.csv')\n",
    "    test = pd.read_csv('./test.csv')\n",
    "    \n",
    "    generator = Chemical_feature_generator()\n",
    "    \n",
    "    def process(df):\n",
    "        molecular_f = [] \n",
    "        for sample in tqdm(df.SMILES):\n",
    "            sample = Chem.MolFromSmiles(sample)\n",
    "            molecular_features = generator.get_molecular_features(mol=sample)\n",
    "            molecular_f.append(molecular_features)\n",
    "\n",
    "        molecular_f = np.concatenate([molecular_f], axis=0)\n",
    "\n",
    "    \n",
    "        return pd.DataFrame(data=molecular_f, columns=['MolWt','HeavyAtomMolWt','NumValenceElectrons','FractionCSP3','HeavyAtomCount',\n",
    "                                                   'NHOHCount','NOCount','NumAliphaticCarbocycles','NumAliphaticHeterocycles','NumAliphaticRings',\n",
    "                                                   'NumAromaticCarbocycles','NumAromaticHeterocycles','NumAromaticRings','NumHAcceptors','NumHDonors',\n",
    "                                                   'NumHeteroatoms','NumRotatableBonds','RingCount','MolMR','CalcNumBridgeheadAtom','ExactMolWt',\n",
    "                                                   'NumRadicalElectrons','NumSaturatedCarbocycles','NumSaturatedHeterocycles','NumSaturatedRings','MolLogP',\n",
    "                                                   'CalcNumAmideBonds','CalcNumSpiroAtoms','num_amion_groups','num_ammonium_groups',\n",
    "                                                   'num_sulfinic_acid_groups','num_alkoxy_groups'])\n",
    "\n",
    "    train_molecular_f = process(train)\n",
    "    train_merged = pd.concat([train, train_molecular_f], axis=1)\n",
    "    \n",
    "    test_molecular_f = process(test)\n",
    "    test_merged = pd.concat([test, test_molecular_f], axis=1)\n",
    "    \n",
    "    train_merged.to_csv('./new_train.csv', index=False)\n",
    "    test_merged.to_csv('./new_test.csv', index=False)\n",
    "    \n",
    "    def deepchem_rdkit(df):\n",
    "        \n",
    "        featurizer = RDKitDescriptors()\n",
    "        rdkit_features = []\n",
    "        \n",
    "        for smiles in tqdm(df.SMILES):\n",
    "            feature = featurizer(smiles)\n",
    "            rdkit_features.append(feature)\n",
    "            \n",
    "        return np.concatenate(rdkit_features)\n",
    "    \n",
    "    \n",
    "    features = deepchem_rdkit(train)\n",
    "    column_means = np.mean(features, axis=0)\n",
    "    non_zero_mean_columns = np.where(column_means != 0)[0]\n",
    "    features = features[:, non_zero_mean_columns]\n",
    "    features = np.concatenate([np.reshape(train.SMILES, (-1, 1)), features], axis=1)\n",
    "    features = pd.DataFrame(features).dropna(axis=1)\n",
    "    pd.DataFrame(features).to_csv('./rdkit_train.csv', index=False)\n",
    "    train_col = features.columns\n",
    "    features = deepchem_rdkit(test)\n",
    "    features = features[:, non_zero_mean_columns]\n",
    "    features = np.concatenate([np.reshape(test.SMILES, (-1 ,1)), features], axis=1)\n",
    "    features = pd.DataFrame(features).dropna(axis=1)\n",
    "    features = features[train_col]\n",
    "    pd.DataFrame(features).to_csv('./rdkit_test.csv', index=False)\n",
    "    test_col = features.columns\n",
    "    \n",
    "    print(list(train_col), len(train_col))\n",
    "    print(list(test_col), len(test_col))\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa125a30",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1198b645",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T02:12:12.699535Z",
     "start_time": "2023-09-23T02:12:01.355122Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[21, 167], edge_index=[2, 48], edge_attr=[48], mol_f=[1, 27], fp=[1, 5235], MLM=[1, 1], HLM=[1, 1], batch=[21], ptr=[2])\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.dataset import IndexType\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from rdkit.Chem import PandasTools\n",
    "\n",
    "                 \n",
    "feature_label = [ 'MolWt', 'HeavyAtomMolWt',\n",
    "                    'NumValenceElectrons', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount',\n",
    "                    'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles',\n",
    "                    'NumAliphaticRings', 'NumAromaticCarbocycles',\n",
    "                    'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors',\n",
    "                    'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'RingCount',\n",
    "                    'MolMR', 'CalcNumBridgeheadAtom', 'ExactMolWt', \n",
    "                    'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles',\n",
    "                    'NumSaturatedRings', 'MolLogP', 'CalcNumAmideBonds',\n",
    "                    'CalcNumSpiroAtoms',  \n",
    "                    'num_ammonium_groups', 'num_sulfinic_acid_groups', 'num_alkoxy_groups'] # 30 \n",
    "\n",
    "\n",
    "given_features = ['AlogP','Molecular_Weight','Num_H_Acceptors','Num_H_Donors','Num_RotatableBonds','LogD','Molecular_PolarSurfaceArea'] # 7 \n",
    "\n",
    "generator = Chemical_feature_generator()\n",
    "\n",
    "class Chemcial_dataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data_frame:pd.DataFrame,\n",
    "                 fps,\n",
    "                 mol_f,\n",
    "                 transform=None,\n",
    "                 is_train=True):\n",
    "        super().__init__()\n",
    "        self.df = data_frame\n",
    "        self.fps = fps\n",
    "        self.mol_f = mol_f\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "            \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.get_chem_prop(idx)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def get_chem_prop(self, idx):\n",
    "        \n",
    "        sample = self.df.iloc[idx]\n",
    "        fingerprint = self.fps[idx]\n",
    "        molecular_feature = self.mol_f[idx]\n",
    "        smiles = sample['SMILES']\n",
    "        \n",
    "        edge_index, edge_attr = generator.get_adj_matrix(smiles=smiles)\n",
    "        atomic_feature = generator.generate_mol_atomic_features(smiles=smiles)\n",
    "        input_ids = generator.encoder_smiles(smiles) # 384\n",
    "\n",
    "\n",
    "        if self.is_train:\n",
    "            MLM = sample['MLM']\n",
    "            HLM = sample['HLM']\n",
    "            atomic_feature = torch.tensor(atomic_feature, dtype=torch.float)\n",
    "            molecular_feature = torch.tensor(molecular_feature, dtype=torch.float).view(1, -1)\n",
    "            fingerprint = torch.tensor(fingerprint, dtype=torch.float).view(1, -1)\n",
    "            MLM = torch.tensor(MLM, dtype=torch.float).view(1, -1)\n",
    "            HLM = torch.tensor(HLM, dtype=torch.float).view(1, -1)\n",
    "            y = torch.concat([MLM, HLM], dim=1)\n",
    "            \n",
    "            return Data(x=atomic_feature, mol_f=molecular_feature, fp=fingerprint,\n",
    "                    edge_index=edge_index, edge_attr=edge_attr, MLM=MLM, HLM=HLM)\n",
    "        else:\n",
    "            atomic_feature = torch.tensor(atomic_feature, dtype=torch.float)\n",
    "            molecular_feature = torch.tensor(molecular_feature, dtype=torch.float).view(1, -1)\n",
    "            fingerprint = torch.tensor(fingerprint, dtype=torch.float).view(1, -1)\n",
    "            \n",
    "            return Data(x=atomic_feature, mol_f=molecular_feature, fp=fingerprint,\n",
    "                    edge_index=edge_index, edge_attr=edge_attr)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "class KFold_pl_DataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 train_df: str = './new_train.csv',\n",
    "                 k_idx: int =1, # fold index\n",
    "                 num_split: int = 5, # fold number, if k=1 then return the whole data\n",
    "                 split_seed: int = 42,\n",
    "                 batch_size: int = 1, \n",
    "                 num_workers: int = 0,\n",
    "                 pin_memory: bool = False,\n",
    "                 persistent_workers: bool=True,\n",
    "                 transform1=None,\n",
    "                 transform2=None\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        persistent_workers = True if num_workers > 0 else False\n",
    "        self.save_hyperparameters(logger=False)\n",
    "        self.transform1 = transform1\n",
    "        self.transform2 = transform2\n",
    "        self.train_data = None\n",
    "        self.val_data = None\n",
    "        self.num_cls = 0\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self, stage=None) -> None:\n",
    "        if not self.train_data and not self.val_data:\n",
    "            df = pd.read_csv(self.hparams.train_df, index_col=0)\n",
    "\n",
    "            mask = df['AlogP'] != df['AlogP']\n",
    "            df.loc[mask, 'AlogP'] = df.loc[mask, 'MolLogP']\n",
    "\n",
    "            PandasTools.AddMoleculeColumnToFrame(df,'SMILES','Molecule')\n",
    "            df[\"FPs\"] = df.Molecule.apply(generator.get_molecule_fingerprints)\n",
    "            train_fps = np.stack(df[\"FPs\"])\n",
    "\n",
    "            feature_selector1 = self.transform1\n",
    "            feature_selector2 = self.transform2\n",
    "            \n",
    "            mol_f = feature_selector1.fit_transform(df[feature_label])\n",
    "            fps = feature_selector2.fit_transform(train_fps)\n",
    "\n",
    "            \n",
    "\n",
    "            kf = KFold(n_splits=self.hparams.num_split,\n",
    "                       shuffle=True,\n",
    "                       random_state=self.hparams.split_seed)\n",
    "            all_splits = [k for k in kf.split(df)]\n",
    "            train_idx, val_idx = all_splits[self.hparams.k_idx]\n",
    "            train_idx, val_idx = train_idx.tolist(), val_idx.tolist()\n",
    "\n",
    "\n",
    "            train_df = df.iloc[train_idx]\n",
    "            train_fp = fps[train_idx]\n",
    "            train_mol_f = mol_f[train_idx]\n",
    "            \n",
    "            val_df = df.iloc[val_idx]\n",
    "            val_fp = fps[val_idx]\n",
    "            val_mol_f = mol_f[val_idx]\n",
    "    \n",
    "\n",
    "            \n",
    "            self.train_data = Chemcial_dataset(data_frame=train_df, fps=train_fp, mol_f=train_mol_f, transform=None, is_train=True)\n",
    "            self.val_data = Chemcial_dataset(data_frame=val_df, fps=val_fp, mol_f=val_mol_f, transform=None, is_train=True)\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data,\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=self.hparams.num_workers,\n",
    "                          persistent_workers=self.hparams.persistent_workers,\n",
    "                          pin_memory=self.hparams.pin_memory,\n",
    "                          drop_last=True)\n",
    "                          \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data,\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=self.hparams.num_workers,\n",
    "                          persistent_workers=self.hparams.persistent_workers,\n",
    "                          pin_memory=self.hparams.pin_memory)\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    feature_selector1 = VarianceThreshold(threshold=0.05)\n",
    "    feature_selector2 = VarianceThreshold(threshold=0.05)\n",
    "    data = KFold_pl_DataModule(transform1=feature_selector1, transform2=feature_selector2)\n",
    "    \n",
    "    train_loader = data.train_dataloader()\n",
    "    val_loader = data.val_dataloader()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # DataBatch(x=[29, 34], edge_index=[2, 62], edge_attr=[62], mol_f=[1, 36], fp=[5235], MLM=[1], HLM=[1], batch=[29], ptr=[2])\n",
    "        print(batch)\n",
    "        break\n",
    "\n",
    "#         DataBatch(x=[22, 167], edge_index=[2, 48], edge_attr=[48], y=[1, 2], mol_f=[1, 7], fp=[1, 300], input_ids=[1, 26], MLM=[1, 1], HLM=[1, 1], batch=[22], ptr=[2])\n",
    "#         DataBatch(x=[19, 167], edge_index=[2, 42], edge_attr=[42], mol_f=[27], fp=[5235], MLM=[1], HLM=[1], batch=[19], ptr=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43479790",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T02:12:12.711432Z",
     "start_time": "2023-09-23T02:12:12.700618Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5235\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print(i.fp.shape[1])\n",
    "    print(i.mol_f.shape[1])\n",
    "    input_size1 = i.fp.shape[1]\n",
    "    input_size2 = i.mol_f.shape[1]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "219bc326",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T02:12:12.714426Z",
     "start_time": "2023-09-23T02:12:12.712504Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "CFG = {'BATCH_SIZE': 256,\n",
    "       'EPOCHS': 200,\n",
    "       'INPUT_SIZE1': input_size1,\n",
    "       'INPUT_SIZE2': input_size2,\n",
    "       'HIDDEN_SIZE': 1024,\n",
    "       'OUTPUT_SIZE': 1,\n",
    "       'DROPOUT_RATE': 0.9,\n",
    "       'LEARNING_RATE': 1e-4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cba50e9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T02:12:14.140088Z",
     "start_time": "2023-09-23T02:12:12.715376Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./new_test.csv', index_col=0)\n",
    "\n",
    "PandasTools.AddMoleculeColumnToFrame(test_df,'SMILES','Molecule')\n",
    "test_df[\"FPs\"] = test_df.Molecule.apply(generator.get_molecule_fingerprints)\n",
    "test_fps = np.stack(test_df[\"FPs\"])\n",
    "\n",
    "test_mol_f = feature_selector1.transform(test_df[feature_label])\n",
    "test_fps = feature_selector2.transform(test_fps)\n",
    "\n",
    "test_idx = test_df.reset_index().index\n",
    "test_idx = list(map(int, test_idx))\n",
    "\n",
    "\n",
    "test_df = test_df.iloc[test_idx]\n",
    "test_fp = test_fps[test_idx]\n",
    "test_mol_f = test_mol_f[test_idx]\n",
    "\n",
    "test_loader = Chemcial_dataset(data_frame=test_df, fps=test_fp, mol_f=test_mol_f, transform=None, is_train=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8CHzFfvrbnOM",
   "metadata": {
    "id": "8CHzFfvrbnOM"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68141433",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T02:12:14.144989Z",
     "start_time": "2023-09-23T02:12:14.141050Z"
    },
    "id": "AWUlJIGf22DO"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size2, hidden_size, dropout_rate, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # fc 레이어 3개와 출력 레이어\n",
    "        self.fc1 = nn.Linear(input_size2, hidden_size) \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # 정규화\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.ln3 = nn.LayerNorm(hidden_size)        \n",
    "        self.ln4 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # 활성화 함수\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):   \n",
    "        out = self.fc1(x)\n",
    "        out = self.ln1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.ln2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        out = self.ln3(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc4(out)\n",
    "        out = self.ln4(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28bfb402",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T02:12:14.150969Z",
     "start_time": "2023-09-23T02:12:14.147399Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size1, hidden_size, dropout_rate, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # fc 레이어 3개와 출력 레이어\n",
    "        self.fc1 = nn.Linear(input_size1, hidden_size)\n",
    "        self.fc2 = nn.GRU(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.RNN(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.RNN(hidden_size, hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # 정규화\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.ln3 = nn.LayerNorm(hidden_size)\n",
    "        self.ln4 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # 활성화 함수\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.ln1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out, _ = self.fc2(out)\n",
    "        out = self.ln2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "#         out, _ = self.fc3(out)\n",
    "#         out = self.ln3(out)\n",
    "#         out = self.activation(out)\n",
    "#         out = self.dropout(out)\n",
    "\n",
    "#         out, _ = self.fc4(out)\n",
    "#         out = self.ln4(out)\n",
    "#         out = self.activation(out)\n",
    "#         out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c175bbca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T02:12:14.154673Z",
     "start_time": "2023-09-23T02:12:14.152010Z"
    }
   },
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, input_size1, input_size2, hidden_size, drop_rate, output_size):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.RNN_extractor = RNN(input_size1, hidden_size, drop_rate, output_size)\n",
    "        self.Net_extractor = Net(input_size2, hidden_size, drop_rate, output_size)\n",
    "#         self.classifier = nn.Linear(in_features=hidden_size*2, out_features=output_size)\n",
    "        self.classifier = nn.Linear(in_features=output_size*2, out_features=output_size)\n",
    "\n",
    "    def forward(self, fp, others):\n",
    "        RNN_feature = self.RNN_extractor(fp)\n",
    "        Net_feature = self.Net_extractor(others)\n",
    "        feature = torch.cat([RNN_feature, Net_feature], dim=-1)\n",
    "        output = self.classifier(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9IlcjfOB22DO",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T02:12:14.325914Z",
     "start_time": "2023-09-23T02:12:14.155698Z"
    },
    "id": "9IlcjfOB22DO"
   },
   "outputs": [],
   "source": [
    "model_MLM = ClassificationModel(CFG['INPUT_SIZE1'],CFG['INPUT_SIZE2'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE'])\n",
    "model_HLM = ClassificationModel(CFG['INPUT_SIZE1'],CFG['INPUT_SIZE2'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72a3b187",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T02:12:14.333815Z",
     "start_time": "2023-09-23T02:12:14.326952Z"
    }
   },
   "outputs": [],
   "source": [
    "# scheduler CosineAnnealingWarmUpRestarts\n",
    "import math\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class CosineAnnealingWarmUpRestarts(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        if T_up < 0 or not isinstance(T_up, int):\n",
    "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.base_eta_max = eta_max\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.T_i = T_0\n",
    "        self.gamma = gamma\n",
    "        self.cycle = 0\n",
    "        self.T_cur = last_epoch\n",
    "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [(self.eta_max - base_lr)*self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.eta_max - base_lr) * (1 + math.cos(math.pi * (self.T_cur-self.T_up) / (self.T_i - self.T_up))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                    self.cycle = epoch // self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.cycle = n\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "                \n",
    "        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "IuQe4Za322DP",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T02:12:14.338495Z",
     "start_time": "2023-09-23T02:12:14.334907Z"
    },
    "id": "IuQe4Za322DP"
   },
   "outputs": [],
   "source": [
    "# criterion = nn.MSELoss()\n",
    "optimizer_MLM = torch.optim.Adam(model_MLM.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "optimizer_HLM = torch.optim.Adam(model_HLM.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "# optimizer_MLM = torch.optim.SGD(model_MLM.parameters(), lr=CFG['LEARNING_RATE'], momentum=0.9)\n",
    "# optimizer_HLM = torch.optim.SGD(model_HLM.parameters(), lr=CFG['LEARNING_RATE'], momentum=0.9)\n",
    "scheduler_MLM = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_MLM, mode='min', factor=0.5, patience=5, threshold_mode='abs', min_lr=0, eps=1e-12, verbose=True)\n",
    "scheduler_HLM = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_HLM, mode='min', factor=0.5, patience=5, threshold_mode='abs', min_lr=0, eps=1e-12, verbose=True)\n",
    "# scheduler_MLM = CosineAnnealingWarmUpRestarts(optimizer_MLM, T_0=20, T_mult=1, eta_max=0.1,  T_up=2, gamma=0.5)\n",
    "# scheduler_HLM = CosineAnnealingWarmUpRestarts(optimizer_HLM, T_0=20, T_mult=1, eta_max=0.1,  T_up=2, gamma=0.5)\n",
    "#주기, 주기변화율, 최대lr, 초기활성ep, 진폭변화율"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e032e346",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97a7f5e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T02:12:14.342032Z",
     "start_time": "2023-09-23T02:12:14.339663Z"
    }
   },
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, y_hat, y):\n",
    "        loss = torch.sqrt(self.mse(y_hat,y))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a17df6b3-16c9-44dd-b0fd-ffb501fee749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T02:12:14.346328Z",
     "start_time": "2023-09-23T02:12:14.342938Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = RMSELoss().to(device)\n",
    "    \n",
    "    \n",
    "    for epoch in range(CFG['EPOCHS']):\n",
    "        running_loss = 0\n",
    "        for loader in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = loader.fp.to(device)\n",
    "            others = loader.mol_f.to(device)\n",
    "            if model==model_MLM:\n",
    "                targets = loader.MLM.to(device)\n",
    "            else:\n",
    "                targets = loader.HLM.to(device)\n",
    "            \n",
    "            output = model(inputs, others)\n",
    "            loss = criterion(output, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            \n",
    "            val_loss = validation(model, criterion, val_loader, device)\n",
    "            \n",
    "            print(f'Epoch: {epoch}, Train Loss: {running_loss/len(train_loader)}, Valid Loss: {val_loss/len(val_loader)}')\n",
    "            \n",
    "            model.train()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "#             if scheduler.num_bad_epochs > scheduler.patience:\n",
    "#                 print(f'Early stopping at epoch {epoch}...')\n",
    "#                 break\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96b1c524-89fb-4ce8-a49f-067fd489f84a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T02:12:14.349950Z",
     "start_time": "2023-09-23T02:12:14.347317Z"
    }
   },
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for loader in train_loader:\n",
    "            inputs = loader.fp.to(device)\n",
    "            others = loader.mol_f.to(device)\n",
    "            if model==model_MLM:\n",
    "                targets = loader.MLM.to(device)\n",
    "            else:\n",
    "                targets = loader.HLM.to(device)\n",
    "            \n",
    "            output = model(inputs, others)\n",
    "            loss = criterion(output, targets)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    \n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eda577c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T10:37:02.061054Z",
     "start_time": "2023-09-23T02:12:14.351157Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Start: MLM\n",
      "Epoch: 0, Train Loss: 33.17528882665324, Valid Loss: 126.15233260699681\n",
      "Epoch 00007: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch: 10, Train Loss: 20.109169371126537, Valid Loss: 72.82078921130726\n",
      "Epoch 00017: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch: 20, Train Loss: 12.04138959802586, Valid Loss: 39.27342323802944\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch: 30, Train Loss: 8.87753287584037, Valid Loss: 25.212926900535823\n",
      "Epoch 00037: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch: 40, Train Loss: 7.7293169177706496, Valid Loss: 19.452408308706115\n",
      "Epoch 00047: reducing learning rate of group 0 to 3.1250e-06.\n",
      "Epoch: 50, Train Loss: 7.3434777118923344, Valid Loss: 16.288244823685716\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.5625e-06.\n",
      "Epoch: 60, Train Loss: 7.003577611464381, Valid Loss: 14.92372656976538\n",
      "Epoch 00067: reducing learning rate of group 0 to 7.8125e-07.\n",
      "Epoch: 70, Train Loss: 6.769226042141045, Valid Loss: 14.418584867194294\n",
      "Epoch 00077: reducing learning rate of group 0 to 3.9063e-07.\n",
      "Epoch: 80, Train Loss: 6.685667412010707, Valid Loss: 13.800266950332693\n",
      "Epoch 00087: reducing learning rate of group 0 to 1.9531e-07.\n",
      "Epoch: 90, Train Loss: 6.816111232507561, Valid Loss: 13.624190246069006\n",
      "Epoch 00097: reducing learning rate of group 0 to 9.7656e-08.\n",
      "Epoch: 100, Train Loss: 6.669946618850814, Valid Loss: 13.489768528459328\n",
      "Epoch 00107: reducing learning rate of group 0 to 4.8828e-08.\n",
      "Epoch: 110, Train Loss: 6.7640046016596065, Valid Loss: 13.486531094387173\n",
      "Epoch 00117: reducing learning rate of group 0 to 2.4414e-08.\n",
      "Epoch: 120, Train Loss: 6.518119394205367, Valid Loss: 13.424959076419473\n",
      "Epoch 00127: reducing learning rate of group 0 to 1.2207e-08.\n",
      "Epoch: 130, Train Loss: 6.809733758312387, Valid Loss: 13.403823449728744\n",
      "Epoch 00137: reducing learning rate of group 0 to 6.1035e-09.\n",
      "Epoch: 140, Train Loss: 6.7404336893519625, Valid Loss: 13.400892472597105\n",
      "Epoch 00147: reducing learning rate of group 0 to 3.0518e-09.\n",
      "Epoch: 150, Train Loss: 6.67228412744273, Valid Loss: 13.403039086876172\n",
      "Epoch 00153: reducing learning rate of group 0 to 1.5259e-09.\n",
      "Epoch 00159: reducing learning rate of group 0 to 7.6294e-10.\n",
      "Epoch: 160, Train Loss: 6.714901942272456, Valid Loss: 13.401628502383828\n",
      "Epoch 00165: reducing learning rate of group 0 to 3.8147e-10.\n",
      "Epoch: 170, Train Loss: 6.690808328000391, Valid Loss: 13.401673686578869\n",
      "Epoch 00171: reducing learning rate of group 0 to 1.9073e-10.\n",
      "Epoch 00177: reducing learning rate of group 0 to 9.5367e-11.\n",
      "Epoch: 180, Train Loss: 6.670288565384225, Valid Loss: 13.401654677082385\n",
      "Epoch 00183: reducing learning rate of group 0 to 4.7684e-11.\n",
      "Epoch 00189: reducing learning rate of group 0 to 2.3842e-11.\n",
      "Epoch: 190, Train Loss: 6.612927904100909, Valid Loss: 13.401651929265686\n",
      "Epoch 00195: reducing learning rate of group 0 to 1.1921e-11.\n",
      "Training Start: HLM\n",
      "Epoch: 0, Train Loss: 43.279865967664996, Valid Loss: 149.18173710823058\n",
      "Epoch 00007: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch: 10, Train Loss: 21.97444456121255, Valid Loss: 78.79234298569816\n",
      "Epoch 00017: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch: 20, Train Loss: 13.576831393226119, Valid Loss: 40.990061490876336\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch: 30, Train Loss: 10.306061679227085, Valid Loss: 26.556335505928313\n",
      "Epoch 00037: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch: 40, Train Loss: 9.191508307360683, Valid Loss: 20.168881514413016\n",
      "Epoch 00047: reducing learning rate of group 0 to 3.1250e-06.\n",
      "Epoch: 50, Train Loss: 8.797162398962229, Valid Loss: 17.52651452251843\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.5625e-06.\n",
      "Epoch: 60, Train Loss: 8.519092834979999, Valid Loss: 16.471073461260115\n",
      "Epoch 00067: reducing learning rate of group 0 to 7.8125e-07.\n",
      "Epoch: 70, Train Loss: 8.268250935203033, Valid Loss: 15.164892032146454\n",
      "Epoch 00077: reducing learning rate of group 0 to 3.9063e-07.\n",
      "Epoch: 80, Train Loss: 8.28424264881507, Valid Loss: 14.983245116983142\n",
      "Epoch 00087: reducing learning rate of group 0 to 1.9531e-07.\n",
      "Epoch: 90, Train Loss: 8.391788982174454, Valid Loss: 14.638615603787558\n",
      "Epoch 00097: reducing learning rate of group 0 to 9.7656e-08.\n",
      "Epoch: 100, Train Loss: 8.333198079611677, Valid Loss: 14.496549866199494\n",
      "Epoch 00107: reducing learning rate of group 0 to 4.8828e-08.\n",
      "Epoch: 110, Train Loss: 8.223980657559279, Valid Loss: 14.4845629797663\n",
      "Epoch 00117: reducing learning rate of group 0 to 2.4414e-08.\n",
      "Epoch: 120, Train Loss: 8.064538557569712, Valid Loss: 14.471354587418693\n",
      "Epoch 00127: reducing learning rate of group 0 to 1.2207e-08.\n",
      "Epoch: 130, Train Loss: 8.457117123784945, Valid Loss: 14.431019584451404\n",
      "Epoch 00137: reducing learning rate of group 0 to 6.1035e-09.\n",
      "Epoch: 140, Train Loss: 8.29287548159769, Valid Loss: 14.423413126128061\n",
      "Epoch 00147: reducing learning rate of group 0 to 3.0518e-09.\n",
      "Epoch: 150, Train Loss: 8.084492191105143, Valid Loss: 14.40334139585495\n",
      "Epoch 00157: reducing learning rate of group 0 to 1.5259e-09.\n",
      "Epoch: 160, Train Loss: 8.18634936852571, Valid Loss: 14.402591633456094\n",
      "Epoch 00167: reducing learning rate of group 0 to 7.6294e-10.\n",
      "Epoch: 170, Train Loss: 8.178399309348196, Valid Loss: 14.403606357574462\n",
      "Epoch 00173: reducing learning rate of group 0 to 3.8147e-10.\n",
      "Epoch 00179: reducing learning rate of group 0 to 1.9073e-10.\n",
      "Epoch: 180, Train Loss: 8.272272956411783, Valid Loss: 14.403481584276472\n",
      "Epoch 00185: reducing learning rate of group 0 to 9.5367e-11.\n",
      "Epoch: 190, Train Loss: 8.205717084448708, Valid Loss: 14.403449447836195\n",
      "Epoch 00191: reducing learning rate of group 0 to 4.7684e-11.\n",
      "Epoch 00197: reducing learning rate of group 0 to 2.3842e-11.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Start: MLM\")\n",
    "model_MLM = train(model_MLM, optimizer_MLM, train_loader, val_loader, scheduler_MLM, device)\n",
    "print(\"Training Start: HLM\")\n",
    "model_HLM = train(model_HLM, optimizer_HLM, train_loader, val_loader, scheduler_HLM, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39bbce5",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5d8a4e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T10:37:02.066452Z",
     "start_time": "2023-09-23T10:37:02.062964Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference(test_loader, model):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for loader in test_loader:\n",
    "            inputs = loader.fp.to(device)\n",
    "            others = loader.mol_f.to(device)\n",
    "            output = model(inputs, others)\n",
    "            preds.extend(output.cpu().numpy().flatten().tolist())\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9794de7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T10:37:12.403377Z",
     "start_time": "2023-09-23T10:37:02.067618Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions_MLM = inference(test_loader, model_MLM)\n",
    "predictions_HLM = inference(test_loader, model_HLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb42d64",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8e574c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T10:37:12.423782Z",
     "start_time": "2023-09-23T10:37:12.404311Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>MLM</th>\n",
       "      <th>HLM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>TEST_478</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>TEST_479</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>TEST_480</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>TEST_481</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>TEST_482</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>483 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  MLM  HLM\n",
       "0    TEST_000    0    0\n",
       "1    TEST_001    0    0\n",
       "2    TEST_002    0    0\n",
       "3    TEST_003    0    0\n",
       "4    TEST_004    0    0\n",
       "..        ...  ...  ...\n",
       "478  TEST_478    0    0\n",
       "479  TEST_479    0    0\n",
       "480  TEST_480    0    0\n",
       "481  TEST_481    0    0\n",
       "482  TEST_482    0    0\n",
       "\n",
       "[483 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('./sample_submission.csv')\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "569d8575",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T10:37:12.435246Z",
     "start_time": "2023-09-23T10:37:12.425060Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>MLM</th>\n",
       "      <th>HLM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>26.235308</td>\n",
       "      <td>57.778999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>78.703026</td>\n",
       "      <td>72.996941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>38.237705</td>\n",
       "      <td>46.721073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>41.198387</td>\n",
       "      <td>69.166420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>38.166775</td>\n",
       "      <td>66.412666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>TEST_478</td>\n",
       "      <td>22.717974</td>\n",
       "      <td>66.544838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>TEST_479</td>\n",
       "      <td>76.779686</td>\n",
       "      <td>93.981628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>TEST_480</td>\n",
       "      <td>29.285145</td>\n",
       "      <td>49.437218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>TEST_481</td>\n",
       "      <td>73.572464</td>\n",
       "      <td>75.679611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>TEST_482</td>\n",
       "      <td>0.819428</td>\n",
       "      <td>30.835888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>483 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id        MLM        HLM\n",
       "0    TEST_000  26.235308  57.778999\n",
       "1    TEST_001  78.703026  72.996941\n",
       "2    TEST_002  38.237705  46.721073\n",
       "3    TEST_003  41.198387  69.166420\n",
       "4    TEST_004  38.166775  66.412666\n",
       "..        ...        ...        ...\n",
       "478  TEST_478  22.717974  66.544838\n",
       "479  TEST_479  76.779686  93.981628\n",
       "480  TEST_480  29.285145  49.437218\n",
       "481  TEST_481  73.572464  75.679611\n",
       "482  TEST_482   0.819428  30.835888\n",
       "\n",
       "[483 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['MLM'] = predictions_MLM\n",
    "submission['HLM'] = predictions_HLM\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2eaf46fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T10:37:12.653273Z",
     "start_time": "2023-09-23T10:37:12.646827Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv('baseline_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb1c65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "804.705872px",
    "left": "213px",
    "top": "118.698524px",
    "width": "211.985291px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "88a5da79f9030d36a713e3ceec9ed9a47a216907c035af9944c458137c4e5cb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
